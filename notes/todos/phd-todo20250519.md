# 19-23 mai 2025: PhD To-Do list

## Tasks
### Current Paper

#### Implementation 
- Construction of the incrementative PHs
- Distance measurements
    - Observation : all heads converge to the same output : 
        Indeed they are solving a convex problem
        - cf : Run Convergence (we plugged successively ten heads to the first block and their cosine similarity all converged to the same layer)
- <del> Destruction and reinit of ~duplicates 
- Test: Plug a two-layers heads to get out of convex problems (Convergence 2layers wandb + PH_accumulations_20mai.py)
    - Observations
        1. Need to change the dimensions for two layers as it take too much space
            - rather than (input, input) and (input, output=2), do (input,512) and (512, 2) 
        2. Layers still converge to become the same, but we end up with less destruction of the rest of the performance and
        3. Their performance don't go up after retraining...
- 3 questions: 
    1. Does this observation 3 hold if we relaunch a head for retraining ?
        - exp 1: relaunch head at the retraining phase (ph lr 0.5)
    3. Does this hold and work without plugging new heads (as anyway, they collapse) ?
        - exp 2: relaunch it without new heads  (ph lr 0.5)
    3. Can we get the same behavior if we successively do it for each block ? 
        - exp 3 (ph lr 0.5): do a script in which 
            - for each block B
                1. we do a full training for block B 
                2. we suppress all the PHs >3
            - Reinit all PHs and train them to completion